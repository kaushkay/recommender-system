{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports for data io operations\n",
    "from collections import deque\n",
    "from six import next\n",
    "import readers\n",
    "\n",
    "# Main imports for training\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate train times per epoch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant seed for replicating training results\n",
    "np.random.seed(42)\n",
    "\n",
    "u_num = 6040 # Number of users in the dataset\n",
    "i_num = 3952 # Number of movies in the dataset\n",
    "\n",
    "batch_size = 1000 # Number of samples per batch\n",
    "dims = 15         # Dimensions of the data, 15\n",
    "max_epochs = 25   # Number of times the network sees all the training data\n",
    "\n",
    "# Device used for all computations\n",
    "place_device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Reads file using the demiliter :: form the ratings file\n",
    "    # Download movie lens data from: http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "    # Columns are user ID, item ID, rating, and timestamp\n",
    "    # Sample data - 3::1196::4::978297539\n",
    "    df = readers.read_file(\"ratings.dat\", sep=\"::\")\n",
    "    rows = len(df)\n",
    "    # Purely integer-location based indexing for selection by position\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    # Separate data into train and test, 90% for train and 10% for test\n",
    "    split_index = int(rows * 0.9)\n",
    "    # Use indices to separate the data\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(user_batch, item_batch, user_num, item_num, dim=5, device=\"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # Using a global bias term\n",
    "        bias_global = tf.get_variable(\"bias_global\", shape=[])\n",
    "        # User and item bias variables\n",
    "        # get_variable: Prefixes the name with the current variable scope \n",
    "        # and performs reuse checks.\n",
    "        w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "        w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "        # embedding_lookup: Looks up 'ids' in a list of embedding tensors\n",
    "        # Bias embeddings for user and items, given a batch\n",
    "        bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "        bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "        # User and item weight variables\n",
    "        w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        # Weight embeddings for user and items, given a batch\n",
    "        embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "        embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    \n",
    "    with tf.device(device):\n",
    "        # reduce_sum: Computes the sum of elements across dimensions of a tensor\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        infer = tf.add(infer, bias_global)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        # l2_loss: Computes half the L2 norm of a tensor without the sqrt\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), \n",
    "                             name=\"svd_regularizer\")\n",
    "    return infer, regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(infer, regularizer, rate_batch, learning_rate=0.1, reg=0.1, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        # Use L2 loss to compute penalty\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        # 'Follow the Regularized Leader' optimizer\n",
    "        # Reference: http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf\n",
    "        train_op = tf.train.FtrlOptimizer(learning_rate).minimize(cost)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 900188, test samples 100021, samples per batch 900\n"
     ]
    }
   ],
   "source": [
    "# Read data from ratings file to build a TF model\n",
    "df_train, df_test = get_data()\n",
    "\n",
    "samples_per_batch = len(df_train) // batch_size\n",
    "print(\"Number of train samples %d, test samples %d, samples per batch %d\" % \n",
    "      (len(df_train), len(df_test), samples_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5411\n",
      "1    5439\n",
      "2     367\n",
      "3     424\n",
      "4    4941\n",
      "Name: user, dtype: int32\n",
      "0    1696\n",
      "1    5448\n",
      "2    2242\n",
      "3    5629\n",
      "4     423\n",
      "Name: user, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Peeking at the top 5 user values\n",
    "print(df_train[\"user\"].head()) \n",
    "print(df_test[\"user\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2682\n",
      "1     903\n",
      "2    3716\n",
      "3    1720\n",
      "4    3696\n",
      "Name: item, dtype: int32\n",
      "0    3113\n",
      "1    1195\n",
      "2     749\n",
      "3    3623\n",
      "4    2899\n",
      "Name: item, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Peeking at the top 5 item values\n",
    "print(df_train[\"item\"].head())\n",
    "print(df_test[\"item\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2.0\n",
      "1    5.0\n",
      "2    4.0\n",
      "3    4.0\n",
      "4    1.0\n",
      "Name: rate, dtype: float32\n",
      "0    5.0\n",
      "1    5.0\n",
      "2    5.0\n",
      "3    2.0\n",
      "4    2.0\n",
      "Name: rate, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# Peeking at the top 5 rate values\n",
    "print(df_train[\"rate\"].head())\n",
    "print(df_test[\"rate\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a shuffle iterator to generate random batches, for training\n",
    "iter_train = readers.ShuffleIterator([df_train[\"user\"],\n",
    "                                     df_train[\"item\"],\n",
    "                                     df_train[\"rate\"]],\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "# Sequentially generate one-epoch batches, for testing\n",
    "iter_test = readers.OneEpochIterator([df_test[\"user\"],\n",
    "                                     df_test[\"item\"],\n",
    "                                     df_test[\"rate\"]],\n",
    "                                     batch_size=-1)\n",
    "\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "infer, regularizer = model(user_batch, item_batch, user_num=u_num, item_num=i_num, dim=dims, device=place_device)\n",
    "_, train_op = loss(infer, regularizer, rate_batch, learning_rate=0.10, reg=0.05, device=place_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain Error\tVal Error\tElapsed Time\n",
      "00\t2.582\t\t2.424\t\t0.339 secs\n",
      "01\t0.954\t\t0.889\t\t14.587 secs\n",
      "02\t0.856\t\t0.874\t\t11.840 secs\n",
      "03\t0.836\t\t0.866\t\t11.785 secs\n",
      "04\t0.821\t\t0.862\t\t11.759 secs\n",
      "05\t0.811\t\t0.859\t\t11.647 secs\n",
      "06\t0.802\t\t0.856\t\t11.656 secs\n",
      "07\t0.796\t\t0.854\t\t11.596 secs\n",
      "08\t0.790\t\t0.853\t\t11.643 secs\n",
      "09\t0.785\t\t0.852\t\t11.500 secs\n",
      "10\t0.781\t\t0.852\t\t11.466 secs\n",
      "11\t0.778\t\t0.851\t\t11.605 secs\n",
      "12\t0.775\t\t0.851\t\t11.661 secs\n",
      "13\t0.772\t\t0.851\t\t11.625 secs\n",
      "14\t0.772\t\t0.851\t\t12.019 secs\n",
      "15\t0.769\t\t0.850\t\t11.688 secs\n",
      "16\t0.767\t\t0.850\t\t6.880 secs\n",
      "17\t0.766\t\t0.850\t\t6.770 secs\n",
      "18\t0.765\t\t0.850\t\t6.756 secs\n",
      "19\t0.764\t\t0.850\t\t6.805 secs\n",
      "20\t0.762\t\t0.850\t\t6.841 secs\n",
      "21\t0.760\t\t0.850\t\t6.884 secs\n",
      "22\t0.760\t\t0.850\t\t6.814 secs\n",
      "23\t0.759\t\t0.850\t\t6.841 secs\n",
      "24\t0.759\t\t0.850\t\t6.728 secs\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    errors = deque(maxlen=samples_per_batch)\n",
    "    start = time.time()\n",
    "    for i in range(max_epochs * samples_per_batch):\n",
    "        users, items, rates = next(iter_train)\n",
    "        _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                               item_batch: items,\n",
    "                                                               rate_batch: rates})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        errors.append(np.power(pred_batch - rates, 2))\n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                        item_batch: items})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "            end = time.time()\n",
    "            \n",
    "            print(\"%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\" % (i // samples_per_batch, train_err, np.sqrt(np.mean(test_err2)), end - start))\n",
    "            start = end\n",
    "\n",
    "    saver.save(sess, './save/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save/model\n",
      "Pred\tActual\n",
      "4.853\t5.000\n",
      "4.646\t5.000\n",
      "5.000\t5.000\n",
      "3.994\t2.000\n",
      "3.009\t2.000\n",
      "4.526\t5.000\n",
      "2.984\t3.000\n",
      "4.047\t4.000\n",
      "1.238\t2.000\n",
      "4.118\t1.000\n",
      "0.849473523206\n"
     ]
    }
   ],
   "source": [
    "# Inference using saved model\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init_op)\n",
    "    new_saver = tf.train.import_meta_graph('./save/model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./save/'))\n",
    "    test_err2 = np.array([])\n",
    "    for users, items, rates in iter_test:\n",
    "        pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                item_batch: items})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        print(\"Pred\\tActual\")\n",
    "        for ii in range(10):\n",
    "            print(\"%.3f\\t%.3f\" % (pred_batch[ii], rates[ii]))\n",
    "        test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "        print(np.sqrt(np.mean(test_err2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
